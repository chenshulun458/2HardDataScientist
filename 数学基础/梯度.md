#Gradient-从底层认知梯度下降
----------

梯度下降-机器学习**优化算法**的核心（gradient descent）

**0-导数**

![image](https://user-images.githubusercontent.com/81349648/120877802-18439200-c5eb-11eb-9320-1de82b957f73.png)

f(x)在x轴上某一点处沿着x轴正方向的变化率

**1-多元函数/偏导数**

在多元函数中，将其他变量看做常数，按照求导法则，计算分别针对某个变量的导数，是偏导数

**2-方向导数**
多元函数在非坐标轴方向也可以求导数

导数-沿坐标轴正方向讨论函数变化率

方向导数-在其他特定方向上的变化率

例：

<img src="https://user-images.githubusercontent.com/81349648/120878648-6f983100-c5f0-11eb-96ae-e7cd5749af8b.png" width="50%">

函数f(x,y) 的A 点在这个方向上也是有切线的，其切线的斜率就是方向导数

**3-梯度**

梯度的提出为了解决一个问题：函数在变量空间的某一点处，沿着哪一个方向有最大的变化率？

答案：函数在某一点的梯度是这样一个**向量**：它的方向与取得**最大方向导数**的方向一致，而它的模为**最大方向导数**的值


在数学中是个抽象概念，从物理的角度更容易理解梯度

梯度表达了在一点的温度与邻近一点的温度间的关系：

考虑两点的温度差：![image](https://user-images.githubusercontent.com/81349648/120885105-b0f10680-c619-11eb-95ff-b7cec7199032.png)

观察到等式右边是两个矢量的点乘：![image](https://user-images.githubusercontent.com/81349648/120885111-c6663080-c619-11eb-9921-c30932e5b86a.png)

我们便可以定义前一个矢量gradT = (Tx', Ty', Tz')。



