#Gradient-从底层认知梯度
----------

**描述**

梯度-多元函数的所有偏导数构成的向量即为梯度

梯度下降-机器学习**优化算法**的核心（gradient descent）

**0-导数**

![image](https://user-images.githubusercontent.com/81349648/120877802-18439200-c5eb-11eb-9320-1de82b957f73.png)

f(x)在x轴上某一点处沿着x轴正方向的变化率

**1-多元函数/偏导数**

在多元函数中，将其他变量看做常数，按照求导法则，计算分别针对某个变量的导数，是偏导数

**2-方向导数**

多元函数在非坐标轴方向也可以求导数

导数-沿坐标轴正方向讨论函数变化率

方向导数-在其他特定方向上的变化率，**方向导数是个标量**！**标量只有大小属性，因此一个点有多个方向导数，这些方向导数必定存在最大值和最小值**

方向导数描述的是，当你在某一点选定一个方向之后，沿着该方向，函数的变化率，因此是标量。

而梯度则是描述某点处选哪个方向方向导数最大，侧重点是方向，所以是一个矢量。

例：

<img src="https://user-images.githubusercontent.com/81349648/120878648-6f983100-c5f0-11eb-96ae-e7cd5749af8b.png" width="50%">

函数f(x,y) 的A 点在这个方向上也是有切线的，其切线的斜率就是方向导数

**方向导数公式**：
![image](https://user-images.githubusercontent.com/81349648/122323344-8e37e980-cf59-11eb-9491-0311c1e9d7f2.png)

从另一个角度理解：各自变量变化对因变量在某一特定方向上的影响。因此偏导数是方向导数的特例

**3-梯度**

梯度的提出为了解决一个问题：函数在变量空间的某一点处，沿着哪一个方向有最大的变化率？

数学上直观的思想是表达出方向导数，然后求最大值。

梯度：函数在某一点的梯度是这样一个**向量**：它的方向与取得**最大方向导数**的方向一致，而它的模为**最大方向导数**的值 （由计算得出）

定义方向导数，在方向l上，有：
![image](https://user-images.githubusercontent.com/81349648/122379328-58b2f080-cf99-11eb-8267-7ced2610008e.png)


我们便可以定义前一个矢量gradT = (Tx', Ty', Tz')为梯度。（多元函数的所有偏导数构成的向量即为梯度）


**补充**：单位向量，模长为1，具有确定方向的向量

引入单位向量之后，方向导数的定义变成了**偏导数**和**单位向量**的内积








