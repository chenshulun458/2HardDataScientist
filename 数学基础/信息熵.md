# Entropy-从底层认知信息熵
--------

**描述**

信息熵-所接受的每条信息中包含信息的平均量（不确定性的量度）

应用：决策树根据熵来往下设置分支（branch）

**0-背景**
希望引入一个量度来**衡量事件的不确定性**。不确定性与热力学中衡量紊乱程度的量度**熵**有着极为相似的含义，因此得名。

**1-期望的性质**

对于这个量度，我们希望它具有：

单调性：发生概率越高的事件，其所携带的信息熵越低 - 单调函数

连续性：该量度应连续，即事件概率值小幅变化只能引起熵的微小变化 - 连续函数

对称性：符号xi重新排序后，该量度应不变：H(p1,p2,...)=H(p2,p1,...)等，即知道一件事情的先后顺序不影响最终信息量 - 乘法交换

存在极值：当所有可能性相等时，具有最大熵，即所有可能的事件同等概率时不确定性最高
![image](https://user-images.githubusercontent.com/81349648/125267530-7aee2300-e339-11eb-9bda-d6da4dbf56d3.png)

可加性：即多随机事件同时发生存在的总不确定性的量度可以表示为各事件不确定性的量度的和 - log函数

**2-信息熵公式**

因此总结出：

![image](https://user-images.githubusercontent.com/81349648/125269553-63b03500-e33b-11eb-89b8-3bc2a8775438.png)

在这里b是对数所使用的底，通常是2，此时单位是bit

**3-举例**

将哪一匹马获胜视为一个随机变量 X->{A,B,C,D}。假定我们需要用尽可能少的二元问题来确定随机变量X的取值。

例如：问题1：A获胜了吗？问题2：B获胜了吗？问题3：C获胜了吗？最后我们可以通过最多3个二元问题，来确定 X 的取值，即哪一匹马赢了比赛。

如果 X=A ，那么需要问1次（问题1：是不是A？），概率为1/2；

如果 X=B ，那么需要问2次（问题1：是不是A？问题2：是不是B？），概率为1/4；

如果 X=C ，那么需要问3次（问题1，问题2，问题3），概率为1/8;

如果 X=D ，那么同样需要问3次（问题1，问题2，问题3），概率为1/16；

因此二元问题数量的期望为：

![image](https://user-images.githubusercontent.com/81349648/125271178-0c12c900-e33d-11eb-9d4f-2ce6dae8ffbd.png)

通过之前的信息熵公式得到：

![image](https://user-images.githubusercontent.com/81349648/125271245-1c2aa880-e33d-11eb-88dd-91ad47e1ce6d.png)

